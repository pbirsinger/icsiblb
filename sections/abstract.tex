
\begin{abstract}

Determing the true accuracy of a multimedia classifier often requires estimating performance on unseen test sets based 
on the performance on a known data set.
This can determine whether a training set amply informs a classifier of the data distribution, or whether high accuracy on a test set is more than a fluke.
Bootstrapping, a statistical method to quantify uncertainty, uncovers the uncertainty of a classifier's performance, yet often demands considerable computational resources, a problem compounded by today's sizable multimedia datasets (citation?). 
Large bootstrapping multimedia applications can become essentially intractable in common productivity languages such as Python and Matlab, as shown later in the paper. 

We propose a methodology to estimate classifier performance that relies on the ability to generate efficient, distributed bootstrapping applications from serial Python. 
This methodology arises from SEJITS (Selective Embedded Just-In-Time Specialization) \cite{Kamil:EECS-2013-1}, an approach to convert programs written in domain-specific languages (DSELs) to programs written in languages suitable for high performance or parallel computation. 
Utilizing an already made DSEL compiler \cite{pbirsinger2013} for a recently developed bootstrapping algorithm, we generate scalable, bootstrapping code suitable for the Spark \cite{zaharia2010spark} cluster computing system. 
This enables us to code in native Python adhering to the predefined DSEL a bootstrapping application that estimates the equal error rate (EER) of a multimedia classifier; the generated code executes in a cluster, garnering orders of magnitude speedup over the naive Python code. 
We then explore the effects of varying the number of positives in both training and test sets obtained from the TRECVID MED 2012 dataset, surveying the flavor of computations now made accessible to non-performance programmers. 

\end{abstract} 