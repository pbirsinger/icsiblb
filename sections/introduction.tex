\section{Introduction}

Estimating a classifier's performance on unseen data sets based on its performance on a known data set can reveal useful information about the training sets, test sets, and classifier. A large confidence interval on the accuracy based on the training data can indicate that the actual accuracy on a test set may deviate wildly from the accuracy on the training set whereas a low standard deviation on the accuracy based on the test data can indicate that the accuracy on other, similar test sets from the same distribution will likely be comparable. 

Fortunately, classifier performance speculation is a well understood problem \cite{fukunaga1989estimation}. 
Extensive investigation of bootstrapping and its variations \cite{efron1979bootstrap,jain1987bootstrap, chernick1985application, sahiner2008classifier} has identified bootstrapping as a prime, ableit not perfect \cite{isaksson2008cross},
solution to this problem. We shift the focus in this paper, however, onto using bootstrapping to estimate the performance of specifically multimedia classifiers. 

When dealing with multimedia data, particularly given the increasing size of modern datasets such as the TRECVID MED 2012 dataset, efficiency becomes paramount. 
We turn to a recently developed bootstrapping method, the Bag of Little Bootstraps (BLB)\cite{kleiner2012big,kleiner2011scalable}, that is designed to run quickly in a distributed setting.
An already existing DSEL compiler converts input Python BLB applications into scalable BLB applications able to run on the Spark cluster computing system \cite{pbirsinger2013}. 
This DSEL compiler makes use of Selective Embedded Just-In-Time Specialization (SEJITS) \cite{Kamil:EECS-2013-1}, an approach for converting DSELs in productivity level languages to high performance efficiency language (e.g. C++ or Cilk) code. 

Here, we describe in this already existing Python DSEL a BLB application which estimates the standard deviation of the equal error rate (EER) of a multimedia classifier. We generate scalable code to run on a cluster to probe the effects of varying training and test set sizes, with data sets from the TRECVID MED 2012 datasest. We investigate the relationship between the training set sizes of these consumer-produced videos and the error estimation on test sets in order to reasonably estimate the necessary minimum size of training sets. We similarly vary the test size to attempt to answer how many consumer-produced videos from this set are needed to provide a reasonable measure of significance to a classification result. 

These experiments characterize our proposed methodology--utilizing a bootstrapping DSEL compiler to generate efficient bootstrapping multimedia classification applications. This methodology now makes accessible an array of multimedia classifier estimation computations for those who never wish to leave the world of Python but previously were forced to, as confirmed by our timing results for naive Python. Moreover, this methodology templates the mechanism through which further useful yet demanding, multimedia-analysis computations can be made attainable for non-performance programmers. 
